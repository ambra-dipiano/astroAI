{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea9c597",
   "metadata": {},
   "source": [
    "# CNN-CLEANER z20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5472641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join, isfile, expandvars\n",
    "from astroai.tools.utils import split_noisy_dataset\n",
    "\n",
    "# data\n",
    "zenith = 'z20' \n",
    "table = 'cleaner_5sgm.pickle'\n",
    "path = f'{expandvars(\"$HOME\")}/E4/irf_{zenith}/crab/'\n",
    "dataset = join(path, table)\n",
    "\n",
    "# models\n",
    "cnnname = 'cleaner_z20'\n",
    "\n",
    "# dataset \n",
    "if '.pickle' in table:\n",
    "    with open(dataset,'rb') as f: ds = pickle.load(f)\n",
    "    infotable = join(path, table.replace('.pickle', '.dat'))\n",
    "    gammatable = join(path, table.replace('.pickle', '_gammapy.txt'))\n",
    "elif '.npy' in table:\n",
    "    ds = np.load(dataset, allow_pickle=True, encoding='latin1', fix_imports=True).flat[0]\n",
    "    infotable = join(path, table.replace('.npy', '.dat'))\n",
    "    gammatable = join(path, table.replace('.npy', '_gammapy.txt'))\n",
    "\n",
    "\n",
    "train_noisy, train_clean, test_noisy, test_clean = split_noisy_dataset(ds, split=80, reshape=True, binning=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c56c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fec6625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_noisy = test_noisy[:1000]\n",
    "len(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acac40fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:15:47.384686: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 16:15:57.551851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-09-19 16:15:57.551882: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-09-19 16:16:08.506322: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-19 16:16:08.506792: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-19 16:16:08.506806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-09-19 16:16:20.332508: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal\n",
      "2024-09-19 16:16:20.332561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kingarthur\n",
      "2024-09-19 16:16:20.332572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kingarthur\n",
      "2024-09-19 16:16:20.332722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.239.6\n",
      "2024-09-19 16:16:20.332760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.239.6\n",
      "2024-09-19 16:16:20.332768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.239.6\n",
      "2024-09-19 16:16:20.333233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 45ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "root = f'{expandvars(\"$HOME\")}/astroAI/astroai/'\n",
    "model = tf.keras.models.load_model(join(root, 'models/crta_models', f'{cnnname}.keras'))\n",
    "predictions = model.predict(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64df369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>seed</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>duration</th>\n",
       "      <th>source_ra</th>\n",
       "      <th>source_dec</th>\n",
       "      <th>point_ra</th>\n",
       "      <th>point_dec</th>\n",
       "      <th>offset</th>\n",
       "      <th>irf</th>\n",
       "      <th>fov</th>\n",
       "      <th>sim_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>crab_00001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>316.641513</td>\n",
       "      <td>12.589385</td>\n",
       "      <td>316.529421</td>\n",
       "      <td>12.904256</td>\n",
       "      <td>0.333312</td>\n",
       "      <td>North_z20_S_0.5h_LST</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.472037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14001</th>\n",
       "      <td>crab_00002</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>16.841739</td>\n",
       "      <td>78.034907</td>\n",
       "      <td>18.877416</td>\n",
       "      <td>77.993683</td>\n",
       "      <td>0.424728</td>\n",
       "      <td>North_z20_0.5h_LST</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.277098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14002</th>\n",
       "      <td>crab_00003</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>340.483309</td>\n",
       "      <td>-17.472847</td>\n",
       "      <td>340.130408</td>\n",
       "      <td>-17.623667</td>\n",
       "      <td>0.368734</td>\n",
       "      <td>North_z20_S_0.5h_LST</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.194221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14003</th>\n",
       "      <td>crab_00004</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>344.849066</td>\n",
       "      <td>86.535858</td>\n",
       "      <td>329.619289</td>\n",
       "      <td>84.942323</td>\n",
       "      <td>1.941093</td>\n",
       "      <td>North_z20_N_0.5h_LST</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.153547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14004</th>\n",
       "      <td>crab_00005</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>214.863376</td>\n",
       "      <td>34.357314</td>\n",
       "      <td>216.119150</td>\n",
       "      <td>33.122741</td>\n",
       "      <td>1.616953</td>\n",
       "      <td>North_z20_N_0.5h_LST</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.129901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  seed  start  stop  duration   source_ra  source_dec  \\\n",
       "14000  crab_00001     1      0   100       100  316.641513   12.589385   \n",
       "14001  crab_00002     2      0   100       100   16.841739   78.034907   \n",
       "14002  crab_00003     3      0   100       100  340.483309  -17.472847   \n",
       "14003  crab_00004     4      0   100       100  344.849066   86.535858   \n",
       "14004  crab_00005     5      0   100       100  214.863376   34.357314   \n",
       "\n",
       "         point_ra  point_dec    offset                   irf  fov  sim_time  \n",
       "14000  316.529421  12.904256  0.333312  North_z20_S_0.5h_LST  2.5  6.472037  \n",
       "14001   18.877416  77.993683  0.424728    North_z20_0.5h_LST  2.5  6.277098  \n",
       "14002  340.130408 -17.623667  0.368734  North_z20_S_0.5h_LST  2.5  6.194221  \n",
       "14003  329.619289  84.942323  1.941093  North_z20_N_0.5h_LST  2.5  6.153547  \n",
       "14004  216.119150  33.122741  1.616953  North_z20_N_0.5h_LST  2.5  6.129901  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infodata = pd.read_csv(infotable, sep=' ', header=0).sort_values(by=['seed'])\n",
    "infodata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f34d67",
   "metadata": {},
   "source": [
    "## Residuals z20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90235bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals = {'STD': [], 'CNN': []}\n",
    "\n",
    "for noisy, clean, pred in zip(test_noisy, test_clean, predictions):\n",
    "    residuals['STD'].append(noisy - clean)\n",
    "    residuals['CNN'].append(noisy - pred)\n",
    "    \n",
    "len(residuals['STD']), len(residuals['CNN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94acbd",
   "metadata": {},
   "source": [
    "## Cumulative FOV counts z20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "032e1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_residual = {'STD': [], 'CNN': []}\n",
    "for std, cnn in zip(residuals['STD'], residuals['CNN']):\n",
    "    sum_residual['STD'].append(np.sum(std))\n",
    "    sum_residual['CNN'].append(np.sum(cnn))\n",
    "\n",
    "sum_fov = {'NOISY': [], 'STD': [], 'CNN': [], 'DIFF': []}\n",
    "for orig, std, cnn in zip(test_noisy, test_clean, predictions):\n",
    "    sum_fov['NOISY'].append(np.sum(orig))\n",
    "    sum_fov['STD'].append(np.sum(std))\n",
    "    sum_fov['CNN'].append(np.sum(cnn)) \n",
    "    sum_fov['DIFF'].append(np.sum(std) - np.sum(cnn))\n",
    "    \n",
    "sum_original_and_diff = {'NOISY': [], 'DIFF': [], 'STD': [], 'CNN': []}\n",
    "for orig, std, cnn in zip(test_noisy, residuals['STD'], residuals['CNN']):\n",
    "    sum_original_and_diff['NOISY'].append(np.sum(orig))\n",
    "    sum_original_and_diff['DIFF'].append(np.sum(std) - np.sum(cnn))\n",
    "    sum_original_and_diff['STD'].append(np.sum(orig - std))\n",
    "    sum_original_and_diff['CNN'].append(np.sum(orig - cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407aeba",
   "metadata": {},
   "source": [
    "## ON excess counts z20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e2281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "from astroai.tools.utils import set_wcs\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "binning = 200\n",
    "pixelsize = (2 * 2.5) / binning\n",
    "point_ref = (binning / 2) + (pixelsize / 2)\n",
    "radius_pix = 0.2/0.025\n",
    "\n",
    "def create_circular_mask(h, w, center=None, radius=None):\n",
    "\n",
    "    if center is None: # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2)\n",
    "\n",
    "    mask = dist_from_center <= radius\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6501f79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['STD', 'CNN', 'AP_EXCESS', 'DIFF'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_on_region = {'STD': [], 'CNN': [], 'AP_EXCESS': [], 'DIFF': []}\n",
    "\n",
    "s = len(train_noisy)\n",
    "for std, cnn in zip(test_clean, predictions):\n",
    "    s += 1 \n",
    "    row = infodata[infodata['seed']==s]\n",
    "    # sky coordinates\n",
    "    source_deg = {'ra': row['source_ra'].values[0], 'dec': row['source_dec'].values[0]}\n",
    "    point_deg = {'ra': row['point_ra'].values[0], 'dec': row['point_dec'].values[0]}\n",
    "    # pixel coordinates\n",
    "    w = set_wcs(point_ra=row['point_ra'].values[0], point_dec=row['point_dec'].values[0], \n",
    "            point_ref=point_ref, pixelsize=pixelsize)\n",
    "    x, y = w.world_to_pixel(SkyCoord(row['source_ra'].values[0], row['source_dec'].values[0], \n",
    "                                                   unit='deg', frame='icrs'))\n",
    "    # ON counts with STD cleaning\n",
    "    h, w = std.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_std = std.copy()\n",
    "    masked_std[~mask] = 0\n",
    "\n",
    "    # ON counts with CNN cleaning\n",
    "    h, w = cnn.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_cnn = cnn.copy()\n",
    "    masked_cnn[~mask] = 0\n",
    "    \n",
    "    sum_on_region['STD'].append(np.sum(masked_std))\n",
    "    sum_on_region['CNN'].append(np.sum(masked_cnn))\n",
    "    sum_on_region['DIFF'].append(np.sum(masked_std - masked_cnn))\n",
    "\n",
    "sum_on_region.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c764a",
   "metadata": {},
   "source": [
    "## Rename z20 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f4d2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_z20 = residuals\n",
    "sum_residual_z20 = sum_residual\n",
    "sum_fov_z20 = sum_fov\n",
    "sum_original_and_diff_z20 = sum_original_and_diff\n",
    "sum_on_region_z20 = sum_on_region\n",
    "\n",
    "with open('data/cleaner_z20_residuals.pickle', 'wb') as f:\n",
    "    pickle.dump(residuals, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_z20_sum_residuals.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_residual, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_z20_sum_fov.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_fov, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_z20_sum_original_and_diff.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_original_and_diff, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_z20_sum_on_region.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_on_region, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625151be",
   "metadata": {},
   "source": [
    "# CNN-cleaner zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff10396f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/dipiano/E4/irf_random/crab/cleaner_5sgm.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_573490/2450677647.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'.pickle'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0minfotable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mgammatable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_gammapy.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/dipiano/E4/irf_random/crab/cleaner_5sgm.pickle'"
     ]
    }
   ],
   "source": [
    "# data\n",
    "zenith = 'random' \n",
    "table = 'cleaner_5sgm.pickle'\n",
    "path = f'{expandvars(\"$HOME\")}/E4/irf_{zenith}/crab/'\n",
    "dataset = join(path, table)\n",
    "\n",
    "# dataset \n",
    "if '.pickle' in table:\n",
    "    with open(dataset,'rb') as f: ds = pickle.load(f)\n",
    "    infotable = join(path, table.replace('.pickle', '.dat'))\n",
    "    gammatable = join(path, table.replace('.pickle', '_gammapy.txt'))\n",
    "elif '.npy' in table:\n",
    "    ds = np.load(dataset, allow_pickle=True, encoding='latin1', fix_imports=True).flat[0]\n",
    "    infotable = join(path, table.replace('.npy', '.dat'))\n",
    "    gammatable = join(path, table.replace('.npy', '_gammapy.txt'))\n",
    "    \n",
    "train_noisy, train_clean, test_noisy, test_clean = split_noisy_dataset(ds, split=80, reshape=True, binning=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b443da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noisy = test_noisy[:1000]\n",
    "len(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c32d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = f'{expandvars(\"$HOME\")}/astroAI/astroai/'\n",
    "model = tf.keras.models.load_model(join(root, 'models/crta_models', f'cleaner_zALL.keras'))\n",
    "predictions = model.predict(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "infodata = pd.read_csv(infotable, sep=' ', header=0).sort_values(by=['seed'])\n",
    "infodata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858868b",
   "metadata": {},
   "source": [
    "## Residuals zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = {'STD': [], 'CNN': []}\n",
    "\n",
    "for noisy, clean, pred in zip(test_noisy, test_clean, predictions):\n",
    "    residuals['STD'].append(noisy - clean)\n",
    "    residuals['CNN'].append(noisy - pred)\n",
    "    \n",
    "len(residuals['STD']), len(residuals['CNN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa3a66",
   "metadata": {},
   "source": [
    "## Cumulative FOV counts zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_residual = {'STD': [], 'CNN': []}\n",
    "for std, cnn in zip(residuals['STD'], residuals['CNN']):\n",
    "    sum_residual['STD'].append(np.sum(std))\n",
    "    sum_residual['CNN'].append(np.sum(cnn))\n",
    "\n",
    "sum_fov = {'NOISY': [], 'STD': [], 'CNN': [], 'DIFF': []}\n",
    "for orig, std, cnn in zip(test_noisy, test_clean, predictions):\n",
    "    sum_fov['NOISY'].append(np.sum(orig))\n",
    "    sum_fov['STD'].append(np.sum(std))\n",
    "    sum_fov['CNN'].append(np.sum(cnn)) \n",
    "    sum_fov['DIFF'].append(np.sum(std) - np.sum(cnn))\n",
    "    \n",
    "sum_original_and_diff = {'NOISY': [], 'DIFF': [], 'STD': [], 'CNN': []}\n",
    "for orig, std, cnn in zip(test_noisy, residuals['STD'], residuals['CNN']):\n",
    "    sum_original_and_diff['NOISY'].append(np.sum(orig))\n",
    "    sum_original_and_diff['DIFF'].append(np.sum(std) - np.sum(cnn))\n",
    "    sum_original_and_diff['STD'].append(np.sum(orig - std))\n",
    "    sum_original_and_diff['CNN'].append(np.sum(orig - cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff948b01",
   "metadata": {},
   "source": [
    "# ON excess counts zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979dc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_on_region = {'STD': [], 'CNN': [], 'DIFF': []}\n",
    "\n",
    "s = len(train_noisy)\n",
    "for std, cnn in zip(test_clean, predictions):\n",
    "    s += 1 \n",
    "    row = infodata[infodata['seed']==s]\n",
    "    # sky coordinates\n",
    "    source_deg = {'ra': row['source_ra'].values[0], 'dec': row['source_dec'].values[0]}\n",
    "    point_deg = {'ra': row['point_ra'].values[0], 'dec': row['point_dec'].values[0]}\n",
    "    # pixel coordinates\n",
    "    w = set_wcs(point_ra=row['point_ra'].values[0], point_dec=row['point_dec'].values[0], \n",
    "            point_ref=point_ref, pixelsize=pixelsize)\n",
    "    x, y = w.world_to_pixel(SkyCoord(row['source_ra'].values[0], row['source_dec'].values[0], \n",
    "                                                   unit='deg', frame='icrs'))\n",
    "    # ON counts with STD cleaning\n",
    "    h, w = std.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_std = std.copy()\n",
    "    masked_std[~mask] = 0\n",
    "\n",
    "    # ON counts with CNN cleaning\n",
    "    h, w = cnn.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_cnn = cnn.copy()\n",
    "    masked_cnn[~mask] = 0\n",
    "    \n",
    "    sum_on_region['STD'].append(np.sum(masked_std))\n",
    "    sum_on_region['CNN'].append(np.sum(masked_cnn))\n",
    "    sum_on_region['DIFF'].append(np.sum(masked_std - masked_cnn))\n",
    "\n",
    "sum_on_region.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6637d2",
   "metadata": {},
   "source": [
    "## Rename zALL vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64931f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_zALL = residuals\n",
    "sum_residual_zALL = sum_residual\n",
    "sum_fov_zALL = sum_fov\n",
    "sum_original_and_diff_zALL = sum_original_and_diff\n",
    "sum_on_region_zALL = sum_on_region\n",
    "\n",
    "with open('data/cleaner_zALL_residuals.pickle', 'wb') as f:\n",
    "    pickle.dump(residuals, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_zALL_sum_residuals.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_residual, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_zALL_sum_fov.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_fov, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_zALL_sum_original_and_diff.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_original_and_diff, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_zALL_sum_on_region.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_on_region, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1159538",
   "metadata": {},
   "source": [
    "# PLOTS z20 vs zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294161f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "radius_deg = 0.2\n",
    "radius_pix = radius_deg/0.025\n",
    "figsize = (10, 10)\n",
    "histsize = (8, 8)\n",
    "fs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25739f06",
   "metadata": {},
   "source": [
    "## RESIDUALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "colors = ['orange', 'navy']\n",
    "labels = ['20° zenith', 'random zenith']\n",
    "hatches = ['\\\\', '//']\n",
    "legends = [mpatches.Patch(facecolor='none', edgecolor=colors[0], hatch=hatches[0]), \n",
    "           mpatches.Patch(facecolor='none', edgecolor=colors[1], hatch=hatches[1])]\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_residual_z20['CNN'], -sum_residual_zALL['CNN'])\n",
    "#ax.set_title('background residuals', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_sum_residuals_z20_vs_zALL.png')\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_original_and_diff_z20['DIFF'], sum_original_and_diff_zALL['DIFF'])\n",
    "#ax.set_title('background residuals', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_bkg_residuals_z20_vs_zALL.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d28c8",
   "metadata": {},
   "source": [
    "## CUMULATIVE FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ebae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['orange', 'navy']\n",
    "labels = ['20° zenith', 'random zenith']\n",
    "hatches = ['\\\\', '//']\n",
    "legends = [mpatches.Patch(facecolor='none', edgecolor=colors[0], hatch=hatches[0]), \n",
    "           mpatches.Patch(facecolor='none', edgecolor=colors[1], hatch=hatches[1])]\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_original_and_diff_z20['NOISY'], sum_original_and_diff_zALL['NOISY'])\n",
    "#ax.set_title('fov counts', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in datatset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid() \n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "#fig.savefig(f'img/paper_cleaner_hist_bkg_residuals_z20_vs_zALL.png')\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_fov_z20['CNN'], sum_fov_zALL['CNN'])\n",
    "#ax.set_title('fov counts', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "#fig.savefig(f'img/paper_cleaner_hist_bkg_residuals_z20_vs_zALL.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353080b",
   "metadata": {},
   "source": [
    "## EXCESS COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['orange', 'navy']\n",
    "labels = ['20° zenith', 'random zenith']\n",
    "hatches = ['\\\\', '//']\n",
    "legends = [mpatches.Patch(facecolor='none', edgecolor=colors[0], hatch=hatches[0]), \n",
    "           mpatches.Patch(facecolor='none', edgecolor=colors[1], hatch=hatches[1])]\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_original_and_diff_z20['DIFF'], sum_original_and_diff_zALL['DIFF']) \n",
    "#ax.set_title('background residuals', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_bkg_residuals_z20_vs_zALL.png')\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_on_region_z20['DIFF'], sum_on_region_zALL['DIFF'])\n",
    "#ax.set_title('source excess', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_excess_z20_vs_zALL.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e205aa",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe78fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_z20 = np.mean(sum_original_and_diff_z20['DIFF'])\n",
    "std_z20 = np.std(sum_original_and_diff_z20['DIFF'])\n",
    "mean_zALL = np.mean(sum_original_and_diff_zALL['DIFF'])\n",
    "std_zALL = np.std(sum_original_and_diff_zALL['DIFF'])\n",
    "\n",
    "mean_z20 = np.mean(sum_on_region_z20['DIFF'])\n",
    "std_z20 = np.std(sum_on_region_z20['DIFF'])\n",
    "mean_zALL = np.mean(sum_on_region_zALL['DIFF'])\n",
    "std_zALL = np.std(sum_on_region_zALL['DIFF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a7592-4419-4f6d-a7be-b4148edf5d56",
   "metadata": {},
   "source": [
    "# PLOTS CNN vs STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'navy']\n",
    "labels = ['standard', 'cnn model']\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_on_region_zALL['STD'], sum_on_region_zALL['CNN'])\n",
    "ax.set_title('source excess', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_residual_zALL['STD'], sum_residual_zALL['CNN'])\n",
    "ax.set_title('background residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca28b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342d157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c449a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b45dfb-c668-4a75-bc59-349d11bc80a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5e120-c48c-4984-a1ed-789d1e8ad473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5dea2c-de3f-4a93-abb7-910156877aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b032a-927a-4475-b27f-a8e08bfc411f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650cf889-923d-4ecb-aead-b711493a90f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astroai",
   "language": "python",
   "name": "astroai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
