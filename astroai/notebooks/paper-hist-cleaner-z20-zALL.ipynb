{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea9c597",
   "metadata": {},
   "source": [
    "# CNN-CLEANER z20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5472641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join, isfile, expandvars\n",
    "from astroai.tools.utils import split_noisy_dataset\n",
    "\n",
    "# data\n",
    "zenith = 'z20' \n",
    "table = 'cleaner_5sgm.pickle'\n",
    "path = f'{expandvars(\"$HOME\")}/E4/irf_{zenith}/crab/'\n",
    "dataset = join(path, table)\n",
    "\n",
    "# models\n",
    "cnnname = 'cleaner_z20'\n",
    "\n",
    "# dataset \n",
    "if '.pickle' in table:\n",
    "    with open(dataset,'rb') as f: ds = pickle.load(f)\n",
    "    infotable = join(path, table.replace('.pickle', '.dat'))\n",
    "    gammatable = join(path, table.replace('.pickle', '_gammapy.txt'))\n",
    "elif '.npy' in table:\n",
    "    ds = np.load(dataset, allow_pickle=True, encoding='latin1', fix_imports=True).flat[0]\n",
    "    infotable = join(path, table.replace('.npy', '.dat'))\n",
    "    gammatable = join(path, table.replace('.npy', '_gammapy.txt'))\n",
    "\n",
    "\n",
    "train_noisy, train_clean, test_noisy, test_clean = split_noisy_dataset(ds, split=80, reshape=True, binning=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c56c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noisy = test_noisy[:1000]\n",
    "len(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "root = f'{expandvars(\"$HOME\")}/astroAI/astroai/'\n",
    "model = tf.keras.models.load_model(join(root, 'models/crta_models', f'{cnnname}.keras'))\n",
    "predictions = model.predict(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64df369",
   "metadata": {},
   "outputs": [],
   "source": [
    "infodata = pd.read_csv(infotable, sep=' ', header=0).sort_values(by=['seed'])\n",
    "infodata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f34d67",
   "metadata": {},
   "source": [
    "## Residuals z20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90235bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = {'STD': [], 'CNN': []}\n",
    "\n",
    "for noisy, clean, pred in zip(test_noisy, test_clean, predictions):\n",
    "    residuals['STD'].append(noisy - clean)\n",
    "    residuals['CNN'].append(noisy - pred)\n",
    "    \n",
    "len(residuals['STD']), len(residuals['CNN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94acbd",
   "metadata": {},
   "source": [
    "## Cumulative FOV counts z20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_residual = {'STD': [], 'CNN': []}\n",
    "for std, cnn in zip(residuals['STD'], residuals['CNN']):\n",
    "    sum_residual['STD'].append(np.sum(std))\n",
    "    sum_residual['CNN'].append(np.sum(cnn))\n",
    "\n",
    "sum_fov = {'NOISY': [], 'STD': [], 'CNN': [], 'DIFF': []}\n",
    "for orig, std, cnn in zip(test_noisy, test_clean, predictions):\n",
    "    sum_fov['NOISY'].append(np.sum(orig))\n",
    "    sum_fov['STD'].append(np.sum(std))\n",
    "    sum_fov['CNN'].append(np.sum(cnn)) \n",
    "    sum_fov['DIFF'].append(np.sum(std) - np.sum(cnn))\n",
    "    \n",
    "sum_original_and_diff = {'NOISY': [], 'DIFF': [], 'STD': [], 'CNN': []}\n",
    "for orig, std, cnn in zip(test_noisy, residuals['STD'], residuals['CNN']):\n",
    "    sum_original_and_diff['NOISY'].append(np.sum(orig))\n",
    "    sum_original_and_diff['DIFF'].append(np.sum(std) - np.sum(cnn))\n",
    "    sum_original_and_diff['STD'].append(np.sum(orig - std))\n",
    "    sum_original_and_diff['CNN'].append(np.sum(orig - cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407aeba",
   "metadata": {},
   "source": [
    "## ON excess counts z20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "from astroai.tools.utils import set_wcs\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "binning = 200\n",
    "pixelsize = (2 * 2.5) / binning\n",
    "point_ref = (binning / 2) + (pixelsize / 2)\n",
    "radius_pix = 0.2/0.025\n",
    "\n",
    "def create_circular_mask(h, w, center=None, radius=None):\n",
    "\n",
    "    if center is None: # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2)\n",
    "\n",
    "    mask = dist_from_center <= radius\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_on_region = {'STD': [], 'CNN': [], 'AP_EXCESS': [], 'DIFF': []}\n",
    "\n",
    "s = len(train_noisy)\n",
    "for std, cnn in zip(test_clean, predictions):\n",
    "    s += 1 \n",
    "    row = infodata[infodata['seed']==s]\n",
    "    # sky coordinates\n",
    "    source_deg = {'ra': row['source_ra'].values[0], 'dec': row['source_dec'].values[0]}\n",
    "    point_deg = {'ra': row['point_ra'].values[0], 'dec': row['point_dec'].values[0]}\n",
    "    # pixel coordinates\n",
    "    w = set_wcs(point_ra=row['point_ra'].values[0], point_dec=row['point_dec'].values[0], \n",
    "            point_ref=point_ref, pixelsize=pixelsize)\n",
    "    x, y = w.world_to_pixel(SkyCoord(row['source_ra'].values[0], row['source_dec'].values[0], \n",
    "                                                   unit='deg', frame='icrs'))\n",
    "    # ON counts with STD cleaning\n",
    "    h, w = std.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_std = std.copy()\n",
    "    masked_std[~mask] = 0\n",
    "\n",
    "    # ON counts with CNN cleaning\n",
    "    h, w = cnn.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_cnn = cnn.copy()\n",
    "    masked_cnn[~mask] = 0\n",
    "    \n",
    "    sum_on_region['STD'].append(np.sum(masked_std))\n",
    "    sum_on_region['CNN'].append(np.sum(masked_cnn))\n",
    "    sum_on_region['DIFF'].append(np.sum(masked_std - masked_cnn))\n",
    "\n",
    "sum_on_region.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c764a",
   "metadata": {},
   "source": [
    "## Rename z20 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_z20 = residuals\n",
    "sum_residual_z20 = sum_residual\n",
    "sum_fov_z20 = sum_fov\n",
    "sum_original_and_diff_z20 = sum_original_and_diff\n",
    "sum_on_region_z20 = sum_on_region\n",
    "\n",
    "with open('data/cleaner_z20_residuals.pickle', 'wb') as f:\n",
    "    pickle.dump(residuals, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_z20_sum_residuals.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_residual, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_z20_sum_fov.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_fov, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_z20_sum_original_and_diff.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_original_and_diff, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_z20_sum_on_region.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_on_region, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625151be",
   "metadata": {},
   "source": [
    "# CNN-cleaner zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "zenith = 'random' \n",
    "table = 'cleaner_5sgm_expALL.npy'\n",
    "path = f'{expandvars(\"$HOME\")}/E4/irf_{zenith}/crab/'\n",
    "dataset = join(path, table)\n",
    "\n",
    "# dataset \n",
    "if '.pickle' in table:\n",
    "    with open(dataset,'rb') as f: ds = pickle.load(f)\n",
    "    infotable = join(path, table.replace('.pickle', '.dat'))\n",
    "    gammatable = join(path, table.replace('.pickle', '_gammapy.txt'))\n",
    "elif '.npy' in table:\n",
    "    ds = np.load(dataset, allow_pickle=True, encoding='latin1', fix_imports=True).flat[0]\n",
    "    infotable = join(path, table.replace('.npy', '.dat'))\n",
    "    gammatable = join(path, table.replace('.npy', '_gammapy.txt'))\n",
    "    \n",
    "train_noisy, train_clean, test_noisy, test_clean = split_noisy_dataset(ds, split=80, reshape=True, binning=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b443da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noisy = test_noisy[:1000]\n",
    "len(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c32d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = f'{expandvars(\"$HOME\")}/astroAI/astroai/'\n",
    "model = tf.keras.models.load_model(join(root, 'models/crta_models', f'cleaner_zALL.keras'))\n",
    "predictions = model.predict(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "infodata = pd.read_csv(infotable, sep=' ', header=0).sort_values(by=['seed'])\n",
    "infodata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858868b",
   "metadata": {},
   "source": [
    "## Residuals zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = {'STD': [], 'CNN': []}\n",
    "\n",
    "for noisy, clean, pred in zip(test_noisy, test_clean, predictions):\n",
    "    residuals['STD'].append(noisy - clean)\n",
    "    residuals['CNN'].append(noisy - pred)\n",
    "    \n",
    "len(residuals['STD']), len(residuals['CNN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa3a66",
   "metadata": {},
   "source": [
    "## Cumulative FOV counts zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_residual = {'STD': [], 'CNN': []}\n",
    "for std, cnn in zip(residuals['STD'], residuals['CNN']):\n",
    "    sum_residual['STD'].append(np.sum(std))\n",
    "    sum_residual['CNN'].append(np.sum(cnn))\n",
    "\n",
    "sum_fov = {'NOISY': [], 'STD': [], 'CNN': [], 'DIFF': []}\n",
    "for orig, std, cnn in zip(test_noisy, test_clean, predictions):\n",
    "    sum_fov['NOISY'].append(np.sum(orig))\n",
    "    sum_fov['STD'].append(np.sum(std))\n",
    "    sum_fov['CNN'].append(np.sum(cnn)) \n",
    "    sum_fov['DIFF'].append(np.sum(std) - np.sum(cnn))\n",
    "    \n",
    "sum_original_and_diff = {'NOISY': [], 'DIFF': [], 'STD': [], 'CNN': []}\n",
    "for orig, std, cnn in zip(test_noisy, residuals['STD'], residuals['CNN']):\n",
    "    sum_original_and_diff['NOISY'].append(np.sum(orig))\n",
    "    sum_original_and_diff['DIFF'].append(np.sum(std) - np.sum(cnn))\n",
    "    sum_original_and_diff['STD'].append(np.sum(orig - std))\n",
    "    sum_original_and_diff['CNN'].append(np.sum(orig - cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff948b01",
   "metadata": {},
   "source": [
    "# ON excess counts zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979dc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_on_region = {'STD': [], 'CNN': [], 'DIFF': []}\n",
    "\n",
    "s = len(train_noisy)\n",
    "for std, cnn in zip(test_clean, predictions):\n",
    "    s += 1 \n",
    "    row = infodata[infodata['seed']==s]\n",
    "    # sky coordinates\n",
    "    source_deg = {'ra': row['source_ra'].values[0], 'dec': row['source_dec'].values[0]}\n",
    "    point_deg = {'ra': row['point_ra'].values[0], 'dec': row['point_dec'].values[0]}\n",
    "    # pixel coordinates\n",
    "    w = set_wcs(point_ra=row['point_ra'].values[0], point_dec=row['point_dec'].values[0], \n",
    "            point_ref=point_ref, pixelsize=pixelsize)\n",
    "    x, y = w.world_to_pixel(SkyCoord(row['source_ra'].values[0], row['source_dec'].values[0], \n",
    "                                                   unit='deg', frame='icrs'))\n",
    "    # ON counts with STD cleaning\n",
    "    h, w = std.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_std = std.copy()\n",
    "    masked_std[~mask] = 0\n",
    "\n",
    "    # ON counts with CNN cleaning\n",
    "    h, w = cnn.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_cnn = cnn.copy()\n",
    "    masked_cnn[~mask] = 0\n",
    "    \n",
    "    sum_on_region['STD'].append(np.sum(masked_std))\n",
    "    sum_on_region['CNN'].append(np.sum(masked_cnn))\n",
    "    sum_on_region['DIFF'].append(np.sum(masked_std - masked_cnn))\n",
    "\n",
    "sum_on_region.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6637d2",
   "metadata": {},
   "source": [
    "## Rename zALL vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64931f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_zALL = residuals\n",
    "sum_residual_zALL = sum_residual\n",
    "sum_fov_zALL = sum_fov\n",
    "sum_original_and_diff_zALL = sum_original_and_diff\n",
    "sum_on_region_zALL = sum_on_region\n",
    "\n",
    "with open('data/cleaner_zALL_residuals.pickle', 'wb') as f:\n",
    "    pickle.dump(residuals, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_zALL_sum_residuals.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_residual, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_zALL_sum_fov.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_fov, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_zALL_sum_original_and_diff.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_original_and_diff, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/cleaner_zALL_sum_on_region.pickle', 'wb') as f:\n",
    "    pickle.dump(sum_on_region, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1159538",
   "metadata": {},
   "source": [
    "# PLOTS z20 vs zALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294161f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "radius_deg = 0.2\n",
    "radius_pix = radius_deg/0.025\n",
    "figsize = (10, 10)\n",
    "histsize = (8, 8)\n",
    "fs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25739f06",
   "metadata": {},
   "source": [
    "## RESIDUALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "colors = ['navy', 'orange']\n",
    "labels = ['20° zenith', 'random zenith']\n",
    "hatches = ['\\\\', '//']\n",
    "legends = [mpatches.Patch(facecolor='none', edgecolor=colors[0], hatch=hatches[0]), \n",
    "           mpatches.Patch(facecolor='none', edgecolor=colors[1], hatch=hatches[1])]\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (np.abs(sum_residual_z20['CNN']), np.abs(sum_residual_zALL['CNN']))\n",
    "#ax.set_title('background residuals', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_sum_residuals_z20_vs_zALL.png')\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_original_and_diff_z20['DIFF'], sum_original_and_diff_zALL['DIFF'])\n",
    "#ax.set_title('background residuals', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_bkg_residuals_z20_vs_zALL.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d28c8",
   "metadata": {},
   "source": [
    "## CUMULATIVE FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ebae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['orange', 'navy']\n",
    "labels = ['20° zenith', 'random zenith']\n",
    "hatches = ['\\\\', '//']\n",
    "legends = [mpatches.Patch(facecolor='none', edgecolor=colors[0], hatch=hatches[0]), \n",
    "           mpatches.Patch(facecolor='none', edgecolor=colors[1], hatch=hatches[1])]\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_original_and_diff_z20['NOISY'], sum_original_and_diff_zALL['NOISY'])\n",
    "#ax.set_title('fov counts', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in datatset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid() \n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_sum_fov_original_z20_vs_zALL.png')\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_fov_z20['CNN'], sum_fov_zALL['CNN'])\n",
    "#ax.set_title('fov counts', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_sum_fov_clean_CNN_z20_vs_zALL.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f70620",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['green', 'maroon']\n",
    "labels = ['original', 'CNN clean']\n",
    "hatches = ['\\\\', '//']\n",
    "legends = [mpatches.Patch(facecolor='none', edgecolor=colors[0], hatch=hatches[0]), \n",
    "           mpatches.Patch(facecolor='none', edgecolor=colors[1], hatch=hatches[1])]\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_original_and_diff_z20['NOISY'], sum_fov_z20['CNN'])\n",
    "#ax.set_title('fov counts', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in datatset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid() \n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_sum_fov_original_and_clean_CNN_z20.png')\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_original_and_diff_zALL['NOISY'], sum_fov_zALL['CNN'])\n",
    "#ax.set_title('fov counts', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_sum_fov_original_and_clean_CNN_zALL.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353080b",
   "metadata": {},
   "source": [
    "## EXCESS COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['orange', 'navy']\n",
    "labels = ['20° zenith', 'random zenith']\n",
    "hatches = ['\\\\', '//']\n",
    "legends = [mpatches.Patch(facecolor='none', edgecolor=colors[0], hatch=hatches[0]), \n",
    "           mpatches.Patch(facecolor='none', edgecolor=colors[1], hatch=hatches[1])]\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_original_and_diff_z20['DIFF'], sum_original_and_diff_zALL['DIFF']) \n",
    "#ax.set_title('background residuals', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_bkg_residuals_z20_vs_zALL.png')\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_on_region_z20['DIFF'], sum_on_region_zALL['DIFF'])\n",
    "#ax.set_title('source excess', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(handles=legends, labels=labels, fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "fig.savefig(f'img/paper_cleaner_hist_excess_z20_vs_zALL.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e205aa",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe78fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_z20 = np.mean(sum_original_and_diff_z20['DIFF'])\n",
    "std_z20 = np.std(sum_original_and_diff_z20['DIFF'])\n",
    "mean_zALL = np.mean(sum_original_and_diff_zALL['DIFF'])\n",
    "std_zALL = np.std(sum_original_and_diff_zALL['DIFF'])\n",
    "\n",
    "mean_z20 = np.mean(sum_on_region_z20['DIFF'])\n",
    "std_z20 = np.std(sum_on_region_z20['DIFF'])\n",
    "mean_zALL = np.mean(sum_on_region_zALL['DIFF'])\n",
    "std_zALL = np.std(sum_on_region_zALL['DIFF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a7592-4419-4f6d-a7be-b4148edf5d56",
   "metadata": {},
   "source": [
    "# PLOTS CNN vs STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'navy']\n",
    "labels = ['standard', 'cnn model']\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_on_region_zALL['STD'], sum_on_region_zALL['CNN'])\n",
    "#ax.set_title('source excess', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()\n",
    "\n",
    "# hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_residual_zALL['STD'], sum_residual_zALL['CNN'])\n",
    "#ax.set_title('background residuals', fontsize=fs*1.5)\n",
    "n, bins, patches = ax.hist(data, 20, density=False, histtype='step', color=colors, label=labels)\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "for patch, hatch in zip(patches, hatches):\n",
    "    plt.setp(patch, hatch=hatch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca28b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342d157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c449a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b45dfb-c668-4a75-bc59-349d11bc80a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5e120-c48c-4984-a1ed-789d1e8ad473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5dea2c-de3f-4a93-abb7-910156877aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b032a-927a-4475-b27f-a8e08bfc411f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650cf889-923d-4ecb-aead-b711493a90f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astroai",
   "language": "python",
   "name": "astroai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
