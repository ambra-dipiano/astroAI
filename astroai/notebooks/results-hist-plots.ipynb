{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f175ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from astroai.tools.utils import set_wcs\n",
    "\n",
    "figsize = (10, 10)\n",
    "histsize = (8, 8)\n",
    "fs = 16\n",
    "\n",
    "fov = 2.5\n",
    "binning = 200\n",
    "pixelsize = (2 * fov) / binning\n",
    "point_ref = (binning / 2) + (pixelsize / 2)\n",
    "radius_pix = 0.2/pixelsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3673deb",
   "metadata": {},
   "source": [
    "# random IRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join, isfile, expandvars\n",
    "\n",
    "# data\n",
    "zenith = 'random'  # 'z20'\n",
    "table = 'cleaner_5sgm.pickle'\n",
    "path = f'{expandvars(\"$HOME\")}/E4/irf_{zenith}/crab/'\n",
    "dataset = join(path, table)\n",
    "\n",
    "# dataset \n",
    "if '.pickle' in table:\n",
    "    with open(dataset,'rb') as f: ds = pickle.load(f)\n",
    "    infotable = join(path, table.replace('.pickle', '.dat'))\n",
    "    gammatable = join(path, table.replace('.pickle', '_gammapy.txt'))\n",
    "elif '.npy' in table:\n",
    "    ds = np.load(dataset, allow_pickle=True, encoding='latin1', fix_imports=True).flat[0]\n",
    "    infotable = join(path, table.replace('.npy', '.dat'))\n",
    "    gammatable = join(path, table.replace('.npy', '_gammapy.txt'))\n",
    "    \n",
    "\n",
    "print(f\"Classes: {ds.keys()}\\n\")\n",
    "print(f\"NOISY dataset size: {len(ds['DS1'])}\")\n",
    "print(f\"CLEAN dataset size: {len(ds['DS2'])}\")\n",
    "\n",
    "ds['DS1'][0].shape, ds['DS2'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroai.tools.utils import split_noisy_dataset\n",
    "\n",
    "train_noisy, train_clean, test_noisy, test_clean = split_noisy_dataset(ds, split=80, reshape=True, binning=200)\n",
    "\n",
    "print(f\"Train clean: {train_clean.shape}\")\n",
    "print(f\"Train noisy: {train_noisy.shape}\")\n",
    "print(f\"\\nTest clean: {test_clean.shape}\")\n",
    "print(f\"Test labenoicyls: {test_noisy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa72340",
   "metadata": {},
   "outputs": [],
   "source": [
    "infodata = pd.read_csv(infotable, sep=' ', header=0).sort_values(by=['seed'])\n",
    "infodata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa2078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "savename = 'cleaner_5sgm_filter12_expRAND' \n",
    "model = tf.keras.models.load_model(f'../models/cnn_cleaner/{savename}.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da535727",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(test_noisy, test_clean, verbose=2)\n",
    "\n",
    "history = np.load(f'../models/cnn_cleaner/{savename}_history.npy', allow_pickle='TRUE').item()\n",
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851b2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=histsize)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "ax.plot(history['loss'], label='training')\n",
    "ax.plot(history['val_loss'], label = 'validation')\n",
    "ax.set_xlabel('Epoch', fontsize=fs)\n",
    "ax.set_ylabel('Loss', fontsize=fs)\n",
    "ax.set_title('loss function', fontsize=fs*1.5)\n",
    "ax.set_ylim([0.35,0.7])\n",
    "ax.grid()\n",
    "ax.legend(fontsize=fs)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'img/talk_rand_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = {'STD': [], 'CNN': []}\n",
    "for noisy, clean, pred in zip(test_noisy, test_clean, predictions):\n",
    "    residuals['STD'].append(noisy - clean)\n",
    "    residuals['CNN'].append(noisy - pred)\n",
    "    \n",
    "sum_residual = {'STD': [], 'CNN': []}\n",
    "for std, cnn in zip(residuals['STD'], residuals['CNN']):\n",
    "    sum_residual['STD'].append(np.sum(std))\n",
    "    sum_residual['CNN'].append(np.sum(cnn))\n",
    "\n",
    "sum_cleaned = {'STD': [], 'CNN': []}\n",
    "for std, cnn in zip(test_clean, predictions):\n",
    "    sum_cleaned['STD'].append(np.sum(std))\n",
    "    sum_cleaned['CNN'].append(np.sum(cnn))\n",
    "    \n",
    "sum_original_and_diff = {'NOISY': [], 'DIFF': []}\n",
    "for orig, std, cnn in zip(test_noisy, residuals['STD'], residuals['CNN']):\n",
    "    sum_original_and_diff['NOISY'].append(np.sum(orig))\n",
    "    sum_original_and_diff['DIFF'].append(np.sum(std - cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae7016",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'blue']\n",
    "\n",
    "# original hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = sum_original_and_diff['NOISY']\n",
    "#ax.set_title('simulations (noisy)', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color='green')\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in datatset', fontsize=fs)\n",
    "ax.set_xlabel('FoV integrated counts', fontsize=fs)\n",
    "ax.grid() \n",
    "fig.savefig(f'img/talk_rand_noisy_counts.png')\n",
    "plt.show()\n",
    "\n",
    "# cumulative counts hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_cleaned['STD'], sum_cleaned['CNN'])\n",
    "#ax.set_title('simulations vs predictions (clean)', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=['simulation', 'prediction'])\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('FoV integrated counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_rand_clean_counts.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7986be",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'blue']\n",
    "\n",
    "# residual hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_residual['STD'], sum_residual['CNN'])\n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=['simulation', 'prediction'])\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('FoV residual counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_rand_residuals_counts.png')\n",
    "plt.show()\n",
    "\n",
    "# diff residual hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = sum_original_and_diff['DIFF']\n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color='g')\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('FoV (simulated - predicted) residual counts', fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_rand_residuals_counts_diff.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8089ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroai.tools.utils import create_circular_mask\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "sum_on_region = {'STD': [], 'CNN': [], 'DIFF': []}\n",
    "\n",
    "s = len(train_noisy)\n",
    "for std, cnn in zip(test_clean, predictions):\n",
    "    s += 1 \n",
    "    row = infodata[infodata['seed']==s]\n",
    "    # sky coordinates\n",
    "    source_deg = {'ra': row['source_ra'].values[0], 'dec': row['source_dec'].values[0]}\n",
    "    point_deg = {'ra': row['point_ra'].values[0], 'dec': row['point_dec'].values[0]}\n",
    "    # pixel coordinates\n",
    "    w = set_wcs(point_ra=row['point_ra'].values[0], point_dec=row['point_dec'].values[0], \n",
    "            point_ref=point_ref, pixelsize=pixelsize)\n",
    "    x, y = w.world_to_pixel(SkyCoord(row['source_ra'].values[0], row['source_dec'].values[0], \n",
    "                                                   unit='deg', frame='icrs'))\n",
    "    # ON counts with STD cleaning\n",
    "    h, w = std.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_std = std.copy()\n",
    "    masked_std[~mask] = 0\n",
    "\n",
    "    # ON counts with CNN cleaning\n",
    "    h, w = cnn.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_cnn = cnn.copy()\n",
    "    masked_cnn[~mask] = 0\n",
    "    \n",
    "    sum_on_region['STD'].append(np.sum(masked_std))\n",
    "    sum_on_region['CNN'].append(np.sum(masked_cnn))\n",
    "    sum_on_region['DIFF'].append(np.sum(masked_std - masked_cnn))\n",
    "\n",
    "sum_on_region.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16425ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'blue'] \n",
    "\n",
    "# cumulative counts hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_on_region['STD'], sum_on_region['CNN']) \n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=['simulation', 'prediction'])\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('excess counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_rand_on_counts.png')\n",
    "plt.show()\n",
    "\n",
    "# diff on counts hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = sum_on_region['DIFF']\n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color='g')\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('(simulated - predicted) excess counts', fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_rand_on_counts_diff.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c767ce4",
   "metadata": {},
   "source": [
    "# z20 IRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5aaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join, isfile\n",
    "\n",
    "# data\n",
    "zenith = 'z20'  # 'random'\n",
    "table = 'cleaner_5sgm_exp30m_z20.pickle'\n",
    "path = f'{expandvars(\"$HOME\")}/E4/irf_{zenith}/crab/'\n",
    "dataset = join(path, table)\n",
    "\n",
    "# dataset \n",
    "if '.pickle' in table:\n",
    "    with open(dataset,'rb') as f: ds = pickle.load(f)\n",
    "    infotable = join(path, table.replace('.pickle', '.dat'))\n",
    "    gammatable = join(path, table.replace('.pickle', '_gammapy.txt'))\n",
    "elif '.npy' in table:\n",
    "    ds = np.load(dataset, allow_pickle=True, encoding='latin1', fix_imports=True).flat[0]\n",
    "    infotable = join(path, table.replace('.npy', '.dat'))\n",
    "    gammatable = join(path, table.replace('.npy', '_gammapy.txt'))\n",
    "    \n",
    "print(f\"Classes: {ds.keys()}\\n\")\n",
    "print(f\"NOISY dataset size: {len(ds['DS1'])}\")\n",
    "print(f\"CLEAN dataset size: {len(ds['DS2'])}\")\n",
    "\n",
    "ds['DS1'][0].shape, ds['DS2'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f11753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroai.tools.utils import split_noisy_dataset\n",
    "\n",
    "train_noisy, train_clean, test_noisy, test_clean = split_noisy_dataset(ds, split=80, reshape=True, binning=200)\n",
    "\n",
    "print(f\"Train clean: {train_clean.shape}\")\n",
    "print(f\"Train noisy: {train_noisy.shape}\")\n",
    "print(f\"\\nTest clean: {test_clean.shape}\")\n",
    "print(f\"Test labenoicyls: {test_noisy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "infodata = pd.read_csv(infotable, sep=' ', header=0).sort_values(by=['seed'])\n",
    "infodata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeada064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "savename = 'cleaner_5sgm_filter12_z20' \n",
    "model = tf.keras.models.load_model(f'../models/cnn_cleaner/{savename}.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50229508",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb21201",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(test_noisy, test_clean, verbose=2)\n",
    "\n",
    "history = np.load(f'../models/cnn_cleaner/{savename}_history.npy', allow_pickle='TRUE').item()\n",
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=histsize)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "ax.plot(history['loss'], label='training')\n",
    "ax.plot(history['val_loss'], label = 'validation')\n",
    "ax.set_xlabel('Epoch', fontsize=fs)\n",
    "ax.set_ylabel('Loss', fontsize=fs)\n",
    "ax.set_title('loss function', fontsize=fs*1.5)\n",
    "ax.set_ylim([0.40,0.60])\n",
    "ax.grid()\n",
    "ax.legend(fontsize=fs)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'img/talk_z20_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_z20 = {'STD': [], 'CNN': []}\n",
    "for noisy, clean, pred in zip(test_noisy, test_clean, predictions):\n",
    "    residuals_z20['STD'].append(noisy - clean)\n",
    "    residuals_z20['CNN'].append(noisy - pred)\n",
    "    \n",
    "sum_residual_z20 = {'STD': [], 'CNN': []}\n",
    "for std, cnn in zip(residuals_z20['STD'], residuals_z20['CNN']):\n",
    "    sum_residual_z20['STD'].append(np.sum(std))\n",
    "    sum_residual_z20['CNN'].append(np.sum(cnn))\n",
    "\n",
    "sum_cleaned_z20 = {'STD': [], 'CNN': []}\n",
    "for std, cnn in zip(test_clean, predictions):\n",
    "    sum_cleaned_z20['STD'].append(np.sum(std))\n",
    "    sum_cleaned_z20['CNN'].append(np.sum(cnn))\n",
    "    \n",
    "sum_original_and_diff_z20 = {'NOISY': [], 'DIFF': []}\n",
    "for orig, std, cnn in zip(test_noisy, residuals_z20['STD'], residuals_z20['CNN']):\n",
    "    sum_original_and_diff_z20['NOISY'].append(np.sum(orig))\n",
    "    sum_original_and_diff_z20['DIFF'].append(np.sum(std - cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b640bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'red']\n",
    "\n",
    "# original hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = sum_original_and_diff_z20['NOISY']\n",
    "#ax.set_title('simulations (noisy)', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color='green')\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in datatset', fontsize=fs)\n",
    "ax.set_xlabel('FoV integrated counts', fontsize=fs)\n",
    "ax.grid() \n",
    "fig.savefig(f'img/talk_z20_noisy_counts.png')\n",
    "plt.show()\n",
    "\n",
    "# cumulative counts hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_cleaned_z20['STD'], sum_cleaned_z20['CNN'])\n",
    "#ax.set_title('simulations vs predictions (clean)', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=['sim', 'cnn'])\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('FoV integrated counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_z20_clean_counts.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bdeb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'red']\n",
    "\n",
    "# residual hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_residual_z20['STD'], sum_residual_z20['CNN'])\n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=['sim', 'cnn'])\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('FoV residual counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_z20_residuals_counts.png')\n",
    "plt.show()\n",
    "\n",
    "# residual diff hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = sum_original_and_diff_z20['DIFF']\n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color='g')\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('FoV (simulated - predicted) residual counts', fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_z20_residuals_counts_diff.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a48df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroai.tools.utils import create_circular_mask\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "sum_on_region_z20 = {'STD': [], 'CNN': [], 'DIFF': []}\n",
    "\n",
    "s = len(train_noisy)\n",
    "for std, cnn in zip(test_clean, predictions):\n",
    "    s += 1 \n",
    "    row = infodata[infodata['seed']==s]\n",
    "    # sky coordinates\n",
    "    source_deg = {'ra': row['source_ra'].values[0], 'dec': row['source_dec'].values[0]}\n",
    "    point_deg = {'ra': row['point_ra'].values[0], 'dec': row['point_dec'].values[0]}\n",
    "    # pixel coordinates\n",
    "    w = set_wcs(point_ra=row['point_ra'].values[0], point_dec=row['point_dec'].values[0], \n",
    "            point_ref=point_ref, pixelsize=pixelsize)\n",
    "    x, y = w.world_to_pixel(SkyCoord(row['source_ra'].values[0], row['source_dec'].values[0], \n",
    "                                                   unit='deg', frame='icrs'))\n",
    "    # ON counts with STD cleaning\n",
    "    h, w = std.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_std = std.copy()\n",
    "    masked_std[~mask] = 0\n",
    "\n",
    "    # ON counts with CNN cleaning\n",
    "    h, w = cnn.shape[:2]\n",
    "    mask = create_circular_mask(h, w, center=(y, x), radius=radius_pix)\n",
    "    masked_cnn = cnn.copy()\n",
    "    masked_cnn[~mask] = 0\n",
    "    \n",
    "    sum_on_region_z20['STD'].append(np.sum(masked_std))\n",
    "    sum_on_region_z20['CNN'].append(np.sum(masked_cnn))\n",
    "    sum_on_region_z20['DIFF'].append(np.sum(masked_std - masked_cnn))\n",
    "\n",
    "sum_on_region_z20.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a85065",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'red'] \n",
    "\n",
    "# cumulative on counts hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_on_region_z20['STD'], sum_on_region_z20['CNN']) \n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=['sim', 'cnn'])\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('excess counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_z20_on_counts.png')\n",
    "plt.show()\n",
    "\n",
    "# diff on counts hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = sum_on_region_z20['DIFF']\n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color='g')\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('(simulated - predicted) excess counts', fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_z20_on_counts_diff.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d353f",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'blue']\n",
    "\n",
    "# residual diff hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_original_and_diff['DIFF'], sum_original_and_diff_z20['DIFF'][:1000])\n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=['random IRF', 'z20 IRF'])\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('FoV (simulated - predicted) residual counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_compare_residuals_counts_diff.png')\n",
    "plt.show()\n",
    "\n",
    "# diff on counts hist\n",
    "fig = plt.figure(figsize=histsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "data = (sum_on_region['DIFF'], sum_on_region_z20['DIFF'][:1000])\n",
    "#ax.set_title('residuals', fontsize=fs*1.5)\n",
    "ax.hist(data, 20, density=False, histtype='step', color=colors, label=['random IRF', 'z20 IRF'])\n",
    "#ax.tick_params(axis='both', labelsize=fs/2)\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('(simulated - predicted) excess counts', fontsize=fs)\n",
    "ax.legend(fontsize=fs)\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f'img/talk_compare_on_counts_diff.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ebe4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f2c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb88450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e722d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd988a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astroai",
   "language": "python",
   "name": "astroai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
