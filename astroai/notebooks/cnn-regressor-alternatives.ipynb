{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b75d0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maps: dict_keys(['DS', 'LABELS'])\n",
      "DS dataset size: 20000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join, isfile, expandvars\n",
    "\n",
    "# data\n",
    "zenith = 'random' # 'z20'  \n",
    "table = 'regressor_5sgm_xy_flip.pickle'\n",
    "path = f'{expandvars(\"$HOME\")}/E4/irf_{zenith}/crab/'\n",
    "dataset = join(path, table)\n",
    "\n",
    "# dataset \n",
    "if '.pickle' in table:\n",
    "    with open(dataset,'rb') as f: ds = pickle.load(f)\n",
    "    infotable = join(path, table.replace('.pickle', '.dat'))\n",
    "elif '.npy' in table:\n",
    "    ds = np.load(dataset, allow_pickle=True, encoding='latin1', fix_imports=True).flat[0]\n",
    "    infotable = join(path, table.replace('.npy', '.dat'))\n",
    "\n",
    "if '_CLEAN' in infotable:\n",
    "    infotable = infotable.replace('_CLEAN', '')\n",
    "    \n",
    "print(f\"Maps: {ds.keys()}\")\n",
    "print(f\"DS dataset size: {len(ds['DS'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627ac6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 200, 200) (16000, 2)\n",
      "Train dataset: 16000, (16000, 200, 200, 1)\n",
      "Train labels: 16000, (16000, 2)\n",
      "\n",
      "Test dataset: 4000, (4000, 200, 200, 1)\n",
      "Test labels: 4000, (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "from astroai.tools.utils import split_regression_dataset\n",
    "\n",
    "binning = 200\n",
    "train_data, train_labels, test_data, test_labels = split_regression_dataset(ds, split=80, reshape=True, binning=binning)\n",
    "\n",
    "print(f\"Train dataset: {len(train_data)}, {train_data.shape}\")\n",
    "print(f\"Train labels: {len(train_labels)}, {train_labels.shape}\")\n",
    "print(f\"\\nTest dataset: {len(test_data)}, {test_data.shape}\")\n",
    "print(f\"Test labels: {len(test_labels)}, {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916fda5",
   "metadata": {},
   "source": [
    "# Load model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858f673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 13:50:58.209944: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-03 13:50:58.326591: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-09-03 13:50:58.326634: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-09-03 13:50:59.630494: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-03 13:50:59.630828: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-03 13:50:59.630840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-09-03 13:51:03.924462: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal\n",
      "2024-09-03 13:51:03.924529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kingarthur\n",
      "2024-09-03 13:51:03.924541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kingarthur\n",
      "2024-09-03 13:51:03.924723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.239.6\n",
      "2024-09-03 13:51:03.924763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.239.6\n",
      "2024-09-03 13:51:03.924772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.239.6\n",
      "2024-09-03 13:51:03.925423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-03 13:51:04.034861: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1060320000 exceeds 10% of free system memory.\n",
      "2024-09-03 13:51:04.129548: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1060320000 exceeds 10% of free system memory.\n",
      "2024-09-03 13:51:04.225550: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1060320000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# model\n",
    "cnn = 'regressor_zALL'\n",
    "\n",
    "model = tf.keras.models.load_model(f'../models/crta_models/{cnn}.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "predictions = model.predict(test_data) * binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36578daa-f7d3-4f3f-b1fe-09e388326b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astroai.tools.utils import plot_heatmap, set_wcs\n",
    "\n",
    "# get random seed\n",
    "idx = np.random.choice(range(len(test_data)))\n",
    "# find seed to get the original heatmap\n",
    "seed = len(train_data) + idx + 1\n",
    "\n",
    "# get simulation data\n",
    "infodata = pd.read_csv(infotable, sep=' ', header=0).sort_values(by=['seed'])\n",
    "row = infodata[infodata['seed']==seed]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025030c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[idx], test_labels[idx] * binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroai.tools.utils import *\n",
    "\n",
    "binning = 200\n",
    "pixelsize = (2 * row['fov'].values[0]) / binning\n",
    "point_ref = (binning / 2) + (pixelsize / 2)\n",
    "w = set_wcs(point_ra=row['point_ra'].values[0], point_dec=row['point_dec'].values[0], \n",
    "            point_ref=point_ref, pixelsize=pixelsize)\n",
    "\n",
    "# TRUE\n",
    "true_sky = SkyCoord(ra=row['source_ra'].values[0], dec=row['source_dec'].values[0], unit='deg', frame='icrs')\n",
    "x, y = w.world_to_pixel(true_sky) \n",
    "true_sky = true_sky.ra.deg, true_sky.dec.deg\n",
    "true_pix = x, y\n",
    "\n",
    "# LABEL\n",
    "label_pix = test_labels[idx][0] * binning, test_labels[idx][1] * binning\n",
    "sky = w.pixel_to_world(label_pix[0], label_pix[1])\n",
    "label_sky = sky.ra.deg, sky.dec.deg\n",
    "\n",
    "# PREDICTION\n",
    "pred_pix = predictions[idx]\n",
    "sky = w.pixel_to_world(pred_pix[0], pred_pix[1])\n",
    "pred_sky = sky.ra.deg, sky.dec.deg\n",
    "\n",
    "\n",
    "print(f'SEED: {seed}')\n",
    "print('---- PIX')\n",
    "print(f\"true: {true_pix}\")\n",
    "print(f\"label: {label_pix}\")\n",
    "print(f\"prediction: {pred_pix}\")\n",
    "print('---- SKY')\n",
    "print(f\"true: {true_sky} \")\n",
    "print(f\"label: {label_sky}\")\n",
    "print(f\"prediction: {pred_sky}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "radius_deg = 0.2\n",
    "radius_pix = radius_deg/0.025\n",
    "figsize = (10, 10)\n",
    "histsize = (8, 8)\n",
    "fs = 16\n",
    "sz = 1.5e3\n",
    "\n",
    "# LEGENDS\n",
    "custom_lines = [Line2D([0], [0], color='k', lw=1, ls='-.'),\n",
    "                Line2D([0], [0], color='r', lw=1, ls='-'),\n",
    "                Line2D([0], [0], color='w', lw=1, ls='--')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9323bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIX\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "img = ax.imshow(test_data[idx], vmin=0, vmax=1)\n",
    "ax.add_patch(plt.Circle((true_pix), radius=radius_pix, edgecolor='k', facecolor='none', ls='-.'))\n",
    "ax.add_patch(plt.Circle((pred_pix), radius=radius_pix, edgecolor='r', facecolor='none', ls='-'))\n",
    "ax.set_ylabel('y [pixels]', fontsize=fs)\n",
    "ax.set_xlabel('x [pixels]', fontsize=fs)\n",
    "ax.set_title(f'counts map coordinates {seed}', fontsize=fs)\n",
    "ax.legend(custom_lines[:2], ['true', 'cnn'], fontsize=fs)\n",
    "\n",
    "cb = fig.colorbar(img, ax=ax, shrink=0.8)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "cb.ax.tick_params(labelsize=fs)\n",
    "cb.set_label('normalised counts', fontsize=fs)\n",
    "\n",
    "fig.savefig(f'{cnn}/regressor_{seed}_pix.png')\n",
    "plt.show()\n",
    "\n",
    "# SKY\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = fig.add_subplot(111, projection=w)\n",
    "\n",
    "img = ax.imshow(test_data[idx], vmin=0, vmax=1)\n",
    "ax.coords['ra'].set_format_unit(u.deg)\n",
    "ax.coords['dec'].set_format_unit(u.deg)\n",
    "ax.scatter(true_sky[0], true_sky[1], transform=ax.get_transform('icrs'), s=sz,\n",
    "           edgecolor='k', facecolor='none', ls='-.')\n",
    "ax.scatter(pred_sky[0], pred_sky[1], transform=ax.get_transform('icrs'), s=sz,\n",
    "           edgecolor='r', facecolor='none', ls='-')\n",
    "ax.set_ylabel('Right Ascension [deg]', fontsize=fs)\n",
    "ax.set_xlabel('RA [deg]', fontsize=fs)\n",
    "ax.set_title(f'counts map {seed}', fontsize=fs)\n",
    "ax.legend(custom_lines[:2], ['true', 'cnn'], fontsize=fs)\n",
    "\n",
    "cb = fig.colorbar(img, ax=ax, shrink=0.8)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "cb.ax.tick_params(labelsize=fs)\n",
    "cb.set_label('normalised counts', fontsize=fs)\n",
    "\n",
    "fig.savefig(f'{cnn}/regressor_{seed}_sky.png')\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b082a18",
   "metadata": {},
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c880dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(test_data, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda75693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = np.load(f'../models/cnn_regressor/{savename}_history.npy', \n",
    "                  allow_pickle='TRUE').item()\n",
    "\n",
    "# LOSS\n",
    "fig, ax = plt.subplots(figsize=histsize)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "ax.plot(history['loss'], label='training')\n",
    "ax.plot(history['val_loss'], label = 'validation')\n",
    "ax.set_xlabel('Epoch', fontsize=fs)\n",
    "ax.set_ylabel('Loss', fontsize=fs)\n",
    "ax.set_title('loss function', fontsize=fs*1.5)\n",
    "ax.set_ylim([0.0,0.2])\n",
    "ax.grid()\n",
    "ax.legend(fontsize=fs)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'{cnn}/regressor_loss_FINAL.png')\n",
    "\n",
    "# ACCURACY\n",
    "fig, ax = plt.subplots(figsize=histsize)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "ax.plot(history['accuracy'], label='training')\n",
    "ax.plot(history['val_accuracy'], label = 'validation')\n",
    "ax.set_xlabel('Epoch', fontsize=fs)\n",
    "ax.set_ylabel('Accuracy', fontsize=fs)\n",
    "ax.set_title('accuracy', fontsize=fs*1.5)\n",
    "ax.set_ylim([0.5,1])\n",
    "ax.grid()\n",
    "ax.legend(fontsize=fs)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'{cnn}/regressor_accuracy_FINAL.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb20d7",
   "metadata": {},
   "source": [
    "# Get separation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a53a5f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE: (132.87109336174686, 56.65261425520772)\n",
      "\n",
      "PREDICTION: (132.9245916091479, 56.57129445076035)\n",
      "\n",
      "ERROR: 0.08648496198050776\n"
     ]
    }
   ],
   "source": [
    "from astroai.tools.utils import *\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "# get true coordinates in SkyCoord\n",
    "true = SkyCoord(ra=row['source_ra'].values[0], dec=row['source_dec'].values[0], unit='deg', frame='icrs')\n",
    "\n",
    "# get errors\n",
    "err = true.separation(sky)\n",
    "\n",
    "print(f\"TRUE: ({true.ra.deg}, {true.dec.deg})\\n\")\n",
    "print(f\"PREDICTION: ({sky.ra.deg}, {sky.dec.deg})\\n\")\n",
    "print(f\"ERROR: {err.deg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de99ba4",
   "metadata": {},
   "source": [
    "# Get DS separation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e21677",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "s = len(train_data)\n",
    "for pred, label in zip(predictions, test_labels):\n",
    "    s += 1 \n",
    "    row = infodata[infodata['seed']==s]\n",
    "    # WCS coordinates\n",
    "    w = set_wcs(point_ra=row['point_ra'].values[0], point_dec=row['point_dec'].values[0], \n",
    "                point_ref=point_ref, pixelsize=pixelsize)\n",
    "    # simulated coordinates\n",
    "    true_deg = {'ra': row['source_ra'].values[0], 'dec': row['source_dec'].values[0]}\n",
    "    true_pix = {'x': label[0], 'y': label[1]}\n",
    "    # prediction coordinates\n",
    "    sky = w.pixel_to_world(pred[0], pred[1])\n",
    "    found_deg = {'ra': sky.ra.deg, 'dec': sky.dec.deg}\n",
    "    # find separation in data\n",
    "    true_sky = SkyCoord(ra=true_deg['ra'], dec=true_deg['dec'], unit='deg', frame='icrs')\n",
    "    found_sky = SkyCoord(ra=found_deg['ra'], dec=found_deg['dec'], unit='deg', frame='icrs')\n",
    "    err.append(true_sky.separation(found_sky))\n",
    "\n",
    "err_noisy = [e.degree for e in err]\n",
    "len(err_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026edcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=histsize)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "\n",
    "# cumulative counts hist\n",
    "data = err_noisy\n",
    "#ax.set_title('reconstruction error', fontsize=fs*1.5)\n",
    "ax.hist(data, 50, density=False, histtype='step', color='g', label=['noisy model'])\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('angular separation (deg)', fontsize=fs)\n",
    "ax.grid()\n",
    "ax.legend(fontsize=fs)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'{cnn}/regressor_loc_error_noisy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd1f7c-90da-4917-bc31-99cac6049cf7",
   "metadata": {},
   "source": [
    "## Statistical measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40677b9-1e5b-452a-9edb-e1e5bf2a12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitter import Fitter, get_common_distributions\n",
    "\n",
    "f = Fitter(err_noisy) # this will fit every distribution available\n",
    "#distributions = ['foldcauchy', 'gibrat', 'halfcauchy', 'kappa3', 'lognorm'] # these 5 are the best fitting \n",
    "#f = Fitter(err_noisy, distributions)\n",
    "f.fit()\n",
    "f.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3086311-e259-4f44-9c70-252433279b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.fitted_param['lognorm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a41e74-88e7-4405-9a79-3dd9d7250580",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = f.fitted_param['lognorm'][-1]\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e87cb0-675b-435d-a5ed-6492f0cd2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=histsize)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "\n",
    "# cumulative counts hist\n",
    "data = err_noisy\n",
    "#ax.set_title('reconstruction error', fontsize=fs*1.5)\n",
    "ax.hist(data, 50, density=False, histtype='step', color='g', label=['noisy model'])\n",
    "ax.axvline(mu, c='k', ls='--', label=fr'$\\mu \\approx$ {np.round(mu, 3)}')\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('angular separation (deg)', fontsize=fs)\n",
    "ax.grid()\n",
    "ax.legend(fontsize=fs)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'{cnn}/regressor_loc_error_noisy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ae637",
   "metadata": {},
   "source": [
    "# Gammapy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158df322",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammafile = infofile.replace('.dat', '_gammapy.txt')\n",
    "gammadata = pd.read_csv(gammafile, sep=' ', header=0).sort_values(by=['seed'])\n",
    "gammadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6507c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "seeds = gammadata['seed']\n",
    "for i, seed in enumerate(seeds):\n",
    "    row = infodata[infodata['seed']==seed]\n",
    "    grow = gammadata[gammadata['seed']==seed]\n",
    "    # WCS coordinates\n",
    "    w = set_wcs(point_ra=row['point_ra'].values[0], point_dec=row['point_dec'].values[0], \n",
    "                point_ref=point_ref, pixelsize=pixelsize)\n",
    "    # simulated coordinates\n",
    "    true_deg = {'ra': row['source_ra'].values[0], 'dec': row['source_dec'].values[0]}\n",
    "    # found coordinates\n",
    "    found_deg = {'ra': grow['loc_ra'].values[0], 'dec': grow['loc_dec'].values[0]}\n",
    "    # find separation in data\n",
    "    true_sky = SkyCoord(ra=true_deg['ra'], dec=true_deg['dec'], unit='deg', frame='icrs')\n",
    "    found_sky = SkyCoord(ra=found_deg['ra'], dec=found_deg['dec'], unit='deg', frame='icrs')\n",
    "    err.append(true_sky.separation(found_sky))\n",
    "    \n",
    "err_gamma = [e.degree for e in err]\n",
    "len(err_noisy), len(err_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (err_noisy[2000:], err_gamma)\n",
    "\n",
    "colors = ['red', 'blue']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=histsize)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "\n",
    "# cumulative counts hist\n",
    "#ax.set_title('localisation error', fontsize=fs*1.5)\n",
    "ax.hist(data, 50, density=False, histtype='step', color=colors, label=['cnn', 'gammapy'])\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('angular separation (deg)', fontsize=fs)\n",
    "ax.grid()\n",
    "ax.legend(fontsize=fs)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'{cnn}/regressor_loc_error_cnn_vs_gp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitter import Fitter, get_common_distributions\n",
    "\n",
    "f = Fitter(err_gamma) # this will fit every distribution available\n",
    "#distributions = ['foldcauchy', 'gibrat', 'halfcauchy', 'kappa3', 'lognorm'] # these 5 are the best fitting \n",
    "#f = Fitter(err_noisy, distributions)\n",
    "f.fit()\n",
    "f.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea15d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_gamma = f.fitted_param['lognorm'][-1]\n",
    "mu_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aca04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (err_noisy[2000:], err_gamma)\n",
    "\n",
    "colors = ['red', 'blue']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=histsize)\n",
    "ax.tick_params(axis='both', labelsize=fs)\n",
    "\n",
    "# cumulative counts hist\n",
    "#ax.set_title('localisation error', fontsize=fs*1.5)\n",
    "ax.hist(data, 50, density=False, histtype='step', color=colors, label=['cnn', 'gammapy'])\n",
    "ax.axvline(mu, c='r', ls='--', label=fr'$\\mu \\approx$ {np.round(mu, 3)}')\n",
    "ax.axvline(mu_gamma, c='b', ls=':', label=fr'$\\mu \\approx$ {np.round(mu_gamma, 3)}')\n",
    "ax.set_ylabel('samples in dataset', fontsize=fs)\n",
    "ax.set_xlabel('angular separation (deg)', fontsize=fs)\n",
    "ax.grid()\n",
    "ax.legend(fontsize=fs)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f'{cnn}/regressor_loc_error_cnn_vs_gp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5309b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/loc_cnn_{zenith}.pickle', 'wb') as f: pickle.dump(err_noisy, f, protocol=4)\n",
    "with open(f'data/loc_gammapy_{zenith}.pickle', 'wb') as f: pickle.dump(err_gamma, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9b1eb-7b2e-43cd-9b38-2555b17a3dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astroai",
   "language": "python",
   "name": "astroai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
